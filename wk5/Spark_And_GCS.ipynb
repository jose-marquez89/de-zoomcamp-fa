{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6779d66b",
   "metadata": {},
   "source": [
    "# Connecting to GCS Using Local Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa5f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d54cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_path = \"/home/jose/Documents/projects/de-zoomcamp-fa/creds/de-zc-i-e4c405955ac3.json\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName(\"test\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", creds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8062702",
   "metadata": {},
   "source": [
    "#### Setting the Spark Context\n",
    "What is the code below doing?\n",
    "- the `fs.` in the `hadoop_conf` variable is saying, \"when you see the file system that starts with 'gs' (GCS), you need to use the google hadoop file system implementation coming from the google cloud connector .jar file\"\n",
    "- AND \"you need to use the service account credentials (third `hadoop_conf` configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f55389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 16:35:05 WARN Utils: Your hostname, jose-MacBookPro resolves to a loopback address: 127.0.1.1; using 192.168.1.224 instead (on interface wlp2s0)\n",
      "23/06/22 16:35:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 16:35:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", creds_path)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc60177",
   "metadata": {},
   "source": [
    "Now, we can create the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2f63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad31c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"gs://dtc_data_lake_de-zc-i/pq/green/*/*\"\n",
    "df_grn = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2875c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2304517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grn.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
